{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is part of a larger project aimed at analyzing the Ames Housing dataset and building predictive models for housing prices. The project is organized into several sections, each focusing on a different aspect of the data analysis and modeling process. The primary goal is to provide a comprehensive understanding of the factors influencing house prices and to develop robust predictive models.\n",
    "\n",
    "## Purpose of This Notebook\n",
    "\n",
    "The purpose of this notebook is to perform data preparation and preprocessing to ensure the dataset is clean and ready for modeling. This involves handling missing values, encoding categorical variables, standardizing numerical features, and removing outliers. These steps are crucial for improving the performance and reliability of the predictive models.\n",
    "\n",
    "**Data Preparation and Preprocessing**\n",
    "\n",
    "**1. Handling Missing Values:**\n",
    "\n",
    "- Identify columns with missing values and their counts.\n",
    "- Impute missing values for numerical columns using the median strategy.\n",
    "- Impute missing values for categorical columns using the most frequent value strategy.\n",
    "- Ensure no missing values remain after imputation.\n",
    "\n",
    "**2. Encoding Categorical Variables:**\n",
    "\n",
    "- Convert categorical variables into dummy/indicator variables using one-hot encoding.\n",
    "\n",
    "**3. Standardizing Numerical Features:**\n",
    "\n",
    "- Standardize all numerical features except 'SalePrice' using StandardScaler.\n",
    "- Scale 'SalePrice' separately to facilitate easier inversion of the scaling during final reporting.\n",
    "\n",
    "**4. Removing Outliers:**\n",
    "\n",
    "- Remove outliers based on GrLivArea to ensure the dataset is not skewed by extreme values.\n",
    "- Specifically, rows where GrLivArea is greater than 4000 square feet are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns with missing values and their counts before preprocessing:\n",
      "Alley           2593\n",
      "MasVnrType      1629\n",
      "MasVnrArea        21\n",
      "BsmtQual          64\n",
      "BsmtCond          64\n",
      "BsmtExposure      67\n",
      "BsmtFinType1      64\n",
      "BsmtFinType2      65\n",
      "Electrical         1\n",
      "FireplaceQu     1277\n",
      "PoolQC          2756\n",
      "Fence           2227\n",
      "MiscFeature     2671\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the Ames dataset\n",
    "file_path = '../data/AmesData.csv'\n",
    "ames_df = pd.read_csv(file_path)\n",
    "\n",
    "# Identify columns with missing values\n",
    "missing_values = ames_df.isnull().sum()\n",
    "print(\"\\nColumns with missing values and their counts before preprocessing:\")\n",
    "print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns with missing values and their counts before preprocessing:\n",
      "Alley           2593\n",
      "MasVnrType      1629\n",
      "MasVnrArea        21\n",
      "BsmtQual          64\n",
      "BsmtCond          64\n",
      "BsmtExposure      67\n",
      "BsmtFinType1      64\n",
      "BsmtFinType2      65\n",
      "Electrical         1\n",
      "FireplaceQu     1277\n",
      "PoolQC          2756\n",
      "Fence           2227\n",
      "MiscFeature     2671\n",
      "dtype: int64\n",
      "\n",
      "Columns with missing values after imputation and their counts:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "The dataset contains 2769 rows and 80 columns before outlier removal.\n",
      "Number of rows removed based on 'GrLivArea' > 4000: 5\n",
      "\n",
      "The dataset contains 2764 rows and 80 columns after outlier removal.\n",
      "\n",
      "The dataset contains 2764 rows and 256 columns after encoding.\n",
      "\n",
      "The dataset contains 2764 rows and 256 columns after scaling.\n"
     ]
    }
   ],
   "source": [
    "# Load the Ames dataset\n",
    "file_path = '../data/AmesData.csv'\n",
    "ames_df = pd.read_csv(file_path)\n",
    "\n",
    "# Specifically replace blanks in 'MasVnrArea' with 0\n",
    "ames_df.replace({'MasVnrArea': 'nan'}, 0, inplace=True)\n",
    "\n",
    "# Identify columns with missing values\n",
    "missing_values = ames_df.isnull().sum()\n",
    "print(\"\\nColumns with missing values and their counts before preprocessing:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Handle missing values for numerical columns\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "num_cols_with_missing = ames_df.select_dtypes(include=[np.number]).columns[ames_df.select_dtypes(include=[np.number]).isnull().any()].tolist()\n",
    "ames_df[num_cols_with_missing] = num_imputer.fit_transform(ames_df[num_cols_with_missing])\n",
    "\n",
    "# Handle missing values for categorical columns\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "cat_cols_with_missing = ames_df.select_dtypes(exclude=[np.number]).columns[ames_df.select_dtypes(exclude=[np.number]).isnull().any()].tolist()\n",
    "ames_df[cat_cols_with_missing] = cat_imputer.fit_transform(ames_df[cat_cols_with_missing])\n",
    "\n",
    "# Ensure no missing values remain\n",
    "print(\"\\nColumns with missing values after imputation and their counts:\")\n",
    "print(ames_df.isnull().sum()[ames_df.isnull().sum() > 0])\n",
    "\n",
    "# Convert boolean columns to integer\n",
    "bool_cols = ames_df.select_dtypes(include=[bool]).columns.tolist()\n",
    "ames_df[bool_cols] = ames_df[bool_cols].astype(int)\n",
    "\n",
    "# Check the number of rows and columns in the dataset before outlier removal\n",
    "num_rows, num_columns = ames_df.shape\n",
    "print(f\"\\nThe dataset contains {num_rows} rows and {num_columns} columns before outlier removal.\")\n",
    "\n",
    "# Remove outliers based on 'GrLivArea'\n",
    "initial_row_count = ames_df.shape[0]\n",
    "ames_df = ames_df.drop(ames_df[ames_df['GrLivArea'] > 4000].index)\n",
    "final_row_count = ames_df.shape[0]\n",
    "print(f\"Number of rows removed based on 'GrLivArea' > 4000: {initial_row_count - final_row_count}\")\n",
    "\n",
    "# Check the number of rows and columns in the dataset after outlier removal\n",
    "num_rows, num_columns = ames_df.shape\n",
    "print(f\"\\nThe dataset contains {num_rows} rows and {num_columns} columns after outlier removal.\")\n",
    "\n",
    "# Encoding Categorical Variables\n",
    "ames_df = pd.get_dummies(ames_df, drop_first=True)\n",
    "\n",
    "# Check the number of rows and columns in the dataset after encoding\n",
    "num_rows, num_columns = ames_df.shape\n",
    "print(f\"\\nThe dataset contains {num_rows} rows and {num_columns} columns after encoding.\")\n",
    "\n",
    "## Note: This scaling didn't work, I couldn't figure out how to invert the scaling after prediction. \n",
    "\n",
    "# Standardize all numerical features\n",
    "# scaler = StandardScaler()\n",
    "# num_features = ames_df.select_dtypes(include=[np.number]).columns\n",
    "# ames_df[num_features] = scaler.fit_transform(ames_df[num_features])\n",
    "\n",
    "#Alternative Approach:\n",
    "# Standardize all numerical features except 'SalePrice'\n",
    "scaler = StandardScaler()\n",
    "num_features = ames_df.select_dtypes(include=[np.number]).columns\n",
    "num_features = num_features.drop('SalePrice')\n",
    "ames_df[num_features] = scaler.fit_transform(ames_df[num_features])\n",
    "\n",
    "# Scale 'SalePrice' separately\n",
    "saleprice_scaler = StandardScaler()\n",
    "ames_df['SalePrice'] = saleprice_scaler.fit_transform(ames_df['SalePrice'].values.reshape(-1, 1))\n",
    "\n",
    "# Check the number of rows and columns in the dataset after scaling\n",
    "num_rows, num_columns = ames_df.shape\n",
    "print(f\"\\nThe dataset contains {num_rows} rows and {num_columns} columns after scaling.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
